<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Yuezhi Yang </title> <meta name="author" content="Yuezhi Yang"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yyuezhi.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuezhi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" target="_blank" href="/assets/pdf/yuezhi_cv.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ARTDECO-480.webp 480w,/assets/img/publication_preview/ARTDECO-800.webp 800w,/assets/img/publication_preview/ARTDECO-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/ARTDECO.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ARTDECO.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ARTDECO2025" class="col-sm-8"> <div class="title"> ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction.</div> <div class="author"> <a href="https://qiminchen.github.io/" rel="external nofollow noopener" target="_blank">Qimin Chen</a> ,  <strong>Yuezhi Yang</strong> ,  <a href="https://yifita.netlify.app/" rel="external nofollow noopener" target="_blank">Yifan Wang</a> ,  <a href="http://www.vovakim.com/" rel="external nofollow noopener" target="_blank">Vladimir G. Kim</a> ,  <a href="https://sidch.com/" rel="external nofollow noopener" target="_blank">Siddhartha Chaudhuri</a> ,  <a href="https://www.cs.sfu.ca/~haoz/" rel="external nofollow noopener" target="_blank">Hao Zhang</a> ,  and  <a href="https://czq142857.github.io/" rel="external nofollow noopener" target="_blank">Zhiqin Chen</a> </div> <div class="periodical"> <em> <span style="color: black;">SIGGRAPH Asia(Conference)</span> </em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/ARTDECO.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a 3D detailizer, a neural model which can instantaneously (in &lt;1s) transform a coarse 3D shape proxy into a high-quality asset with detailed geometry and texture as guided by an input text prompt. Our model is trained using the text prompt, which defines the shape class and characterizes the appearance and fine-grained style of the generated details. The coarse 3D proxy, which can be easily varied and adjusted (e.g., via user editing), provides structure control over the final shape. Importantly, our detailizer is not optimized for a single shape; it is the result of distilling a generative model, so that it can be reused, without retraining, to generate any number of shapes, with varied structures, whose local details all share a consistent style and appearance. Our detailizer training utilizes a pretrained multi-view image diffusion model, with text conditioning, to distill the foundational knowledge therein into our detailizer via Score Distillation Sampling (SDS). To improve SDS and enable our detailizer architecture to learn generalizable features over complex structures, we train our model in two training stages to generate shapes with increasing structural complexity. Through extensive experiments, we show that our method generates shapes of superior quality and details compared to existing text-to-3D models under varied structure control. Our detailizer can refine a coarse shape in less than a second, making it possible to interactively author and adjust 3D shapes. Furthermore, the user-imposed structure control can lead to creative, and hence out-of-distribution, 3D asset generations that are beyond the current capabilities of leading text-to-3D generative models. We demonstrate an interactive 3D modeling workflow our method enables, and its strong generalizability over styles, structures, and object categories.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/SHAlign-480.webp 480w,/assets/img/publication_preview/SHAlign-800.webp 800w,/assets/img/publication_preview/SHAlign-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/SHAlign.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SHAlign.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SHAlign2025" class="col-sm-8"> <div class="title"> An Efficient Global-to-Local Rotation Optimization Approach via Spherical Harmonics.</div> <div class="author"> <a href="https://hzh16.github.io/" rel="external nofollow noopener" target="_blank">Zihang He</a> ,  <strong>Yuezhi Yang</strong> ,  <a href="https://cs.stanford.edu/~congyue/" rel="external nofollow noopener" target="_blank">Congyue Deng</a> ,  <a href="https://jiaxin-lu.github.io/" rel="external nofollow noopener" target="_blank">Jiaxin Lu</a> ,  <a href="https://geometry.stanford.edu/" rel="external nofollow noopener" target="_blank">Leonidas Guibas</a> ,  and  <a href="https://www.cs.utexas.edu/~huangqx/" rel="external nofollow noopener" target="_blank">Qixing Huang</a> </div> <div class="periodical"> <em> <span style="color: black;">Symposium on Geometry Processing (SGP)</span> </em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/SHAlign.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper studies the classical problem of 3D shape alignment, namely computing the relative rotation between two shapes (centered at the origin and normalized by scale) by aligning spherical harmonic coefficients of their spherical function representations. Unlike most prior work, which focuses on the regime in which the inputs have approximately the same shape, we focus on the more general and challenging setting in which the shapes may differ. Central to our approach is a stability analysis of spherical harmonic coefficients, which sheds light on how to align them for robust rotation estimation. We observe that due to symmetries, certain spherical harmonic coefficients may vanish. As a result, using a robust norm for alignment that automatically discards such coefficients offers more accurate rotation estimates than the widely used L2 norm. To enable efficient continuous optimization, we show how to analytically compute the Jacobian of spherical harmonic coefficients with respect to rotations. We also introduce an efficient approach for rotation initialization that requires only a sparse set of rotation samples. Experimental results show that our approach achieves better accuracy and efficiency compared to baseline approaches.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GenAnalysis-480.webp 480w,/assets/img/publication_preview/GenAnalysis-800.webp 800w,/assets/img/publication_preview/GenAnalysis-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/GenAnalysis.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GenAnalysis.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="GenAnalysis2025" class="col-sm-8"> <div class="title"> GenAnalysis: Joint Shape Analysis by Learning Man-Made Shape Generators with Deformation Regularizations.</div> <div class="author"> <strong>Yuezhi Yang</strong> ,  <a href="https://yanghtr.github.io/" rel="external nofollow noopener" target="_blank">Haitao Yang</a> ,  <a href="https://georgenakayama.github.io/" rel="external nofollow noopener" target="_blank">George Kiyohiro Nakayama</a> ,  <a href="https://en.westlake.edu.cn/faculty/xiangru-huang.html/" rel="external nofollow noopener" target="_blank">Xiangru Huang</a> ,  <a href="https://geometry.stanford.edu/" rel="external nofollow noopener" target="_blank">Leonidas Guibas</a> ,  and  <a href="https://www.cs.utexas.edu/~huangqx/" rel="external nofollow noopener" target="_blank">Qixing Huang</a> </div> <div class="periodical"> <em> <span style="color: black;">SIGGRAPH (Journal)</span> </em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/GenAnalysis_small.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="GenAnalysis/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We introduce GenAnalysis, an implicit shape generation framework that allows joint analysis of a collection of man-made shapes. GenAnalysis innovates in learning an implicit shape generator to reconstruct a continuous shape space from the input shape collection. It offers interpolations between pairs of input shapes for correspondence computation. It also allows us to understand the shape variations of each shape in the context of neighboring shapes. Such variations provide segmentation cues. A key idea of GenAnalysis is to enforce an as-affine-as-possible (AAAP) deformation regularization loss among adjacent synthetic shapes of the generator. This loss forces the generator to learn the underlying piece-wise affine part structures. We show how to extract data-driven segmentation cues by recovering piece-wise affine vector fields in the tangent space of each shape and how to use this generator to compute consistent inter-shape correspondences. These correspondences are then used to aggregate single-shape segmentation cues into consistent segmentations. Experimental results on benchmark datasets show that GenAnalysis achieves state-of-the-art results on shape segmentation and shape matching.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GenVDM-480.webp 480w,/assets/img/publication_preview/GenVDM-800.webp 800w,/assets/img/publication_preview/GenVDM-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/GenVDM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GenVDM.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="GenVDM2025" class="col-sm-8"> <div class="title"> GenVDM: Generating Vector Displacement Maps From a Single Image.</div> <div class="author"> <strong>Yuezhi Yang</strong> ,  <a href="https://qiminchen.github.io/" rel="external nofollow noopener" target="_blank">Qimin Chen</a> ,  <a href="http://www.vovakim.com/" rel="external nofollow noopener" target="_blank">Vladimir G. Kim</a> ,  <a href="https://sidch.com/" rel="external nofollow noopener" target="_blank">Siddhartha Chaudhuri</a> ,  <a href="https://www.cs.utexas.edu/~huangqx/" rel="external nofollow noopener" target="_blank">Qixing Huang</a> ,  and  <a href="https://czq142857.github.io/" rel="external nofollow noopener" target="_blank">Zhiqin Chen</a> </div> <div class="periodical"> <em> <span style="color: black;">Conference on Computer Vision and Pattern Recognition (CVPR)</span> <span style="color: red;">highlight</span> </em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/GenVDM.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=QnLVobyZUuM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/yyuezhi/GenVDM/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="GenVDM/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Vector displacement map is a common tool for artists to add parts or surface details to 3D shapes. However, creating VDM is tedious as it requires artists enormous effort to model 3D shapes, Despite the growing popularity of text/image to 3D object model, most of them has fail to generate details comparable to VDM. In this project, we present a system to automatically generate a VDM conditioned on text prompt or an image. To tackle the challenge of VDM data scarcity, we propose a multi-view normal image generation model that fineturn stable diffusion model on part-level objaverse data. We then build a reconstruction model to directly reconstruct VDM from multi-view data. Preliminary result shows that our model outperforms text/image to 3D models in detail generation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sketchconcept-480.webp 480w,/assets/img/publication_preview/sketchconcept-800.webp 800w,/assets/img/publication_preview/sketchconcept-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/sketchconcept.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sketchconcept.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SketchConcept2022" class="col-sm-8"> <div class="title">Discovering Design Concepts for CAD Sketches</div> <div class="author"> <strong>Yuezhi Yang</strong> ,  and  <a href="https://haopan.github.io/" rel="external nofollow noopener" target="_blank">Hao Pan</a> </div> <div class="periodical"> <em> <span style="color: black;">Advances in Neural Information Processing Systems (NeurIPS)</span> <span style="color: red;">spotlight</span> </em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/sketchconcept.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://neurips.cc/virtual/2022/poster/54432" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/yyuezhi/SketchConcept" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="SketchConcept/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Sketch design concepts are recurring patterns found in parametric CAD sketches.Though rarely explicitly formalized by the CAD designers, these concepts are implicitly used in design for modularity and regularity. In this paper, we propose a learning based approach that discovers the modular concepts by induction over raw sketches. We propose the dual implicit-explicit representation of concept structures that allows implicit detection and explicit generation, and the separation of structure generation and parameter instantiation for parameterized concept generation, to learn modular concepts by end-to-end training. We demonstrate the design concept learning on a large scale CAD sketch dataset and show its applications for design intent interpretation and auto-completion.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/toothimpaintor-480.webp 480w,/assets/img/publication_preview/toothimpaintor-800.webp 800w,/assets/img/publication_preview/toothimpaintor-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/toothimpaintor.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="toothimpaintor.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ToothImpaintor2021" class="col-sm-8"> <div class="title">ToothInpaintor: Tooth Inpainting from Partial 3D Dental Model and 2D Panoramic Image</div> <div class="author"> <strong>Yuezhi Yang</strong> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Zhiming Cui</a> ,  <a href="https://enigma-li.github.io/" rel="external nofollow noopener" target="_blank">Changjian Li</a> ,  and  <a href="https://www.cs.hku.hk/people/academic-staff/wenping" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em> <span style="color: black;">Arxiv</span> </em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/toothimpaintor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In orthodontic treatment, a full tooth model consisting of both the crown and root is indispensable in making the treatment plan. However, acquiring tooth root information to obtain the full tooth model from CBCT images is sometimes restricted due to the massive radiation of CBCT scanning. Thus, reconstructing the full tooth shape from the ready-to-use input, e.g., the partial intra-oral scan and the 2D panoramic image, is an applicable and valuable solution. In this paper, we propose a neural network, called ToothInpaintor, that takes as input a partial 3D dental model and a 2D panoramic image and reconstructs the full tooth model with high-quality root(s). Technically, we utilize the implicit rep- resentation for both the 3D and 2D inputs, and learn a latent space of the full tooth shapes. At test time, given an input, we successfully project it to the learned latent space via neural optimization to obtain the full tooth model conditioned on the input. To help find the robust projec- tion, a novel adversarial learning module is exploited in our pipeline. We extensively evaluate our method on a dataset collected from real-world clinics. The evaluation, comparison, and comprehensive ablation studies demonstrate that our approach </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/DomainGeneralization-480.webp 480w,/assets/img/publication_preview/DomainGeneralization-800.webp 800w,/assets/img/publication_preview/DomainGeneralization-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/DomainGeneralization.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DomainGeneralization.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DomainGeneralization2021" class="col-sm-8"> <div class="title">Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning</div> <div class="author"> <a href="https://www.cs.hku.hk/people/academic-staff/wenping" rel="external nofollow noopener" target="_blank">Zheren Li</a> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Zhiming Cui</a> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Sheng Wang</a> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Yuji Qi</a> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Xi Ouyang</a> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Qitian Chen</a> ,  <strong>Yuezhi Yang</strong> ,  <a href="https://erdanc.github.io/" rel="external nofollow noopener" target="_blank">Zhong Xue</a> , and <span class="more-authors" title="click to view 2 more authors" onclick=" var e=this,t=e.textContent==`2 more authors`?`Dinggang Shen, Jie-Zhi Cheng`:`2 more authors`, c=0; var id=setInterval(()=&gt;{e.textContent=t.slice(0,++c); if(c==t.length)clearInterval(id);}, '10'); "> 2 more authors </span> </div> <div class="periodical"> <em> <span style="color: black;">Medical Image Computing and Computer Assisted Intervention Conference(MICCAI)</span> </em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/DomainGeneralization.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> Lesion detection is a fundamental problem in the computeraided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuezhi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>